{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\riyag\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the intents and pre-trained model\n",
    "with open(\"intents.json\", \"r\") as file:\n",
    "    intents = json.load(file)\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through the Intents to Convert them to words, classes, documents, and ignore_words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Looping through the Intents to Convert them to words, classes, documents, and ignore_words.\")\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming, Lowering and Removing Duplicates.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(\"Stemming, Lowering and Removing Duplicates.\")\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming, Lowering and Removing Duplicates.\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemming, Lowering and Removing Duplicates.\")\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 documents\n",
      "140 classes ['None', 'advanced_coding_practices', 'advanced_data_science_techniques', 'advanced_learning_strategies', 'agile_project_management', 'appointment', 'book_genre_recommendation', 'book_recommendation', 'building_customer_relationships', 'building_personal_brand', 'building_resilience_skills', 'cancel_order', 'career_advice', 'career_opportunities', 'coding_help', 'coding_resources', 'community_involvement', 'company_values', 'contact', 'cooking_tutorials', 'creative_problem_solving_approaches', 'creative_urban_planning', 'creative_writing_tips', 'cultural_intelligence_development', 'customer_feedback', 'customer_privacy', 'customization', 'digital_marketing_strategies', 'discount', 'education', 'effective_business_networking', 'effective_communication_skills', 'effective_conflict_resolution', 'effective_crisis_management', 'effective_health_and_wellness_routines', 'effective_interpersonal_communication', 'effective_learning_management', 'effective_online_learning', 'effective_online_marketing', 'effective_project_management', 'effective_remote_leadership', 'effective_team_collaboration', 'effective_time_management', 'emotional_intelligence_development', 'entrepreneurial_guidance', 'ethical_decision_making', 'events', 'exploring_career_switch', 'faq', 'feedback', 'financial_advice', 'financial_planning', 'fitness_motivation', 'fitness_tips', 'gardening_tips', 'gift_options', 'gift_return', 'goal_setting_strategies', 'goodbye', 'greeting', 'health_tips', 'home_decor', 'hours', 'inclusive_team_culture', 'innovative_product_design', 'international_shipping', 'job_search', 'joke', 'language_learning', 'leadership_innovation_strategies', 'learning_new_skills', 'location', 'lost_item', 'lost_password', 'loyalty_program', 'mastering_data_analysis', 'membership', 'membership_benefits', 'membership_tiers', 'mindful_consumption_practices', 'mindful_leadership', 'mindfulness_practices', 'mopeds', 'movie_recommendation', 'music_recommendation', 'nutrition_and_wellness', 'opentoday', 'order_status', 'order_tracking', 'parenting_advice', 'partnership', 'payments', 'personal_financial_investment', 'pet_advice', 'photography_tips', 'podcast_recommendation', 'positive_mindset_cultivation', 'positive_parenting_approaches', 'pre_order', 'price_match', 'product_availability', 'product_care', 'product_comparison', 'product_info', 'product_launch', 'product_recommendation', 'product_reviews', 'product_warranty', 'productive_work_habits', 'public_speaking_skills', 'recipe_suggestions', 'recommendation', 'relationship_advice', 'remote_work_productivity', 'rental', 'responsible_technology_use', 'returns', 'self_care_tips', 'shipping', 'size_guide', 'sizing_assistance', 'social_media', 'stress_management_techniques', 'study_tips', 'subscription', 'sustainability', 'sustainable_living_practices', 'tech_support', 'technical_support', 'technology_recommendations', 'technology_trends', 'thanks', 'today', 'travel_recommendation', 'travel_tips', 'virtual_assistant', 'virtual_event_planning', 'weather', 'website_help', 'yoga_practices']\n",
      "435 unique stemmed words [\"'d\", \"'m\", \"'s\", ',', '--', 'a', 'about', 'acceiv', 'account', 'achiev', 'acquir', 'address', 'adopt', 'adv', 'agil', 'an', 'ana', 'and', 'any', 'anyon', 'ap', 'apply', 'appoint', 'approach', 'ar', 'assist', 'at', 'avail', 'bal', 'becom', 'begin', 'being', 'belong', 'benefit', 'best', 'block', 'book', 'brand', 'budget', 'build', 'busy', 'bye', 'campaign', 'can', 'cancel', 'cap', 'car', 'card', 'cash', 'challeng', 'chang', 'channel', 'chart', 'check', 'cho', 'choo', 'city', 'cod', 'coh', 'collab', 'commun', 'comp', 'company', 'compel', 'complex', 'conceiv', 'conduc', 'conflict', 'connect', 'cons', 'conscy', 'consid', 'cont', 'contact', 'cook', 'country', 'cov', 'cre', 'credit', 'cri', 'cross-cultural', 'cult', 'cur', 'custom', 'customer-centric', 'dai', 'dat', 'day', 'deadlin', 'dec', 'decid', 'decision-making', 'delivery', 'delv', 'describ', 'design', 'destin', 'develop', 'diff', 'digit', 'dilemm', 'din', 'direct', 'discount', 'dish', 'div', 'do', 'doe', 'eat', 'eco-friendly', 'educ', 'effect', 'effort', 'email', 'emot', 'enh', 'entrepr', 'environ', 'equ', 'est', 'eth', 'ev', 'excel', 'exerc', 'expert', 'expl', 'feat', 'fee', 'feedback', 'field', 'fin', 'find', 'fit', 'foc', 'for', 'forgot', 'fost', 'found', 'fright', 'fut', 'gadget', 'gard', 'genr', 'get', 'gift', 'giv', 'go', 'goal', 'goal-setting', 'going', 'good', 'goodby', 'guar', 'guid', 'habit', 'handl', 'hav', 'heal', 'healthy', 'hello', 'help', 'hi', 'hol', 'hom', 'host', 'hour', 'how', 'hunt', 'i', 'ide', 'impact', 'impl', 'improv', 'in', 'includ', 'incorp', 'individ', 'inform', 'innov', 'inquiry', 'inspir', 'instruct', 'intellig', 'intern', 'interperson', 'interview', 'into', 'invest', 'involv', 'is', 'issu', 'it', 'item', 'job', 'join', 'jok', 'kind', 'know', 'landscap', 'langu', 'lat', 'latest', 'laugh', 'launch', 'lead', 'learn', 'leav', 'level', 'lif', 'lifestyl', 'lik', 'list', 'liv', 'loc', 'long', 'look', 'lost', 'loy', 'maint', 'maintain', 'mak', 'man', 'market', 'mast', 'mastercard', 'mat', 'match', 'me', 'med', 'meet', 'memb', 'ment', 'methodolog', 'mind', 'mindset', 'miss', 'money', 'mop', 'mor', 'mot', 'movy', 'multipl', 'mus', 'my', 'navig', 'nee', 'neg', 'network', 'new', 'now', 'numb', 'nutrit', 'of', 'off', 'on', 'onlin', 'op', 'opportun', 'opt', 'optim', 'or', 'ord', 'oth', 'outreach', 'overcom', 'pack', 'par', 'parent-child', 'partn', 'password', 'path', 'perk', 'person', 'pet', 'philosoph', 'phon', 'photograph', 'phy', 'plan', 'podcast', 'policy', 'pop', 'portfolio', 'posit', 'pract', 'pre', 'pre-order', 'pre-ordering', 'prep', 'pric', 'principl', 'priorit', 'priv', 'problem', 'problem-solving', 'process', 'produc', 'profess', 'proficy', 'program', 'project', 'promot', 'protect', 'provid', 'publ', 'purcha', 'quot', 'rain', 'read', 'rec', 'recip', 'recommend', 'recovery', 'reduc', 'rel', 'reliev', 'remot', 'rent', 'reset', 'resilience-building', 'resolv', 'resourc', 'respon', 'rest', 'resy', 'return', 'review', 'reward', 'right', 'routin', 'sav', 'schedule', 'sci', 'scop', 'search', 'see', 'sel', 'self-care', 'serv', 'set', 'setback', 'shar', 'ship', 'shop', 'should', 'sit', 'siz', 'skil', 'soc', 'solv', 'som', 'spac', 'speak', 'spec', 'stag', 'start', 'startup', 'stat', 'stay', 'step', 'stock', 'strategies', 'stress', 'stress-free', 'strong', 'struggle', 'study', 'subject', 'subscrib', 'success', 'suggest', 'support', 'sustain', 'switch', 'tak', 'task', 'team', 'teamwork', 'tech', 'techn', 'technolog', 'tel', 'thank', 'that', 'the', 'ther', 'thi', 'think', 'thought', 'through', 'tier', 'tim', 'tip', 'to', 'today', 'toward', 'track', 'transit', 'travel', 'trend', 'trick', 'trip', 'tut', 'understand', 'upcom', 'upgrad', 'urb', 'us', 'vac', 'valu', 'vibr', 'virt', 'want', 'warranty', 'watch', 'we', 'wea', 'websit', 'wel', 'welcom', 'well-being', 'what', 'when', 'wher', 'which', 'whil', 'with', 'within', 'work', 'workout', 'wrap', 'writ', 'yo', 'yog', 'you']\n",
      "Creating the Data for our Model.\n",
      "Creating a List (Empty) for Output.\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique stemmed words\", words)\n",
    "\n",
    "print(\"Creating the Data for our Model.\")\n",
    "training = []\n",
    "output = []\n",
    "print(\"Creating a List (Empty) for Output.\")\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Training Set, Bag of Words for our Model.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Training Set, Bag of Words for our Model.\")\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for the current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling Randomly and Converting into Numpy Array for Faster Processing.\n",
      "Creating Train and Test Lists.\n",
      "Building Neural Network for Our Chatbot to be Contextual.\n",
      "Resetting graph data.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing.\")\n",
    "random.shuffle(training)\n",
    "\n",
    "# Separate bags and output_rows into separate lists\n",
    "bags, output_rows = zip(*training)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_x = np.array(bags)\n",
    "train_y = np.array(output_rows)\n",
    "\n",
    "print(\"Creating Train and Test Lists.\")\n",
    "# Convert numpy arrays to lists\n",
    "train_x = list(train_x)\n",
    "train_y = list(train_y)\n",
    "\n",
    "\n",
    "print(\"Building Neural Network for Our Chatbot to be Contextual.\")\n",
    "print(\"Resetting graph data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch 1/150\n",
      "54/54 [==============================] - 1s 2ms/step - loss: 4.9476 - accuracy: 0.0024\n",
      "Epoch 2/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.9442 - accuracy: 0.0047\n",
      "Epoch 3/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.9418 - accuracy: 0.0047\n",
      "Epoch 4/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.9397 - accuracy: 0.0094\n",
      "Epoch 5/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.9371 - accuracy: 0.0094\n",
      "Epoch 6/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.9341 - accuracy: 0.0165\n",
      "Epoch 7/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.9308 - accuracy: 0.0141\n",
      "Epoch 8/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.9272 - accuracy: 0.0141\n",
      "Epoch 9/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.9218 - accuracy: 0.0141\n",
      "Epoch 10/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.9168 - accuracy: 0.0165\n",
      "Epoch 11/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.9093 - accuracy: 0.0188\n",
      "Epoch 12/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.9006 - accuracy: 0.0235\n",
      "Epoch 13/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.8905 - accuracy: 0.0282\n",
      "Epoch 14/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.8769 - accuracy: 0.0353\n",
      "Epoch 15/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.8619 - accuracy: 0.0212\n",
      "Epoch 16/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.8409 - accuracy: 0.0188\n",
      "Epoch 17/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.8162 - accuracy: 0.0329\n",
      "Epoch 18/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.7873 - accuracy: 0.0259\n",
      "Epoch 19/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.7510 - accuracy: 0.0306\n",
      "Epoch 20/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.7136 - accuracy: 0.0306\n",
      "Epoch 21/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.6713 - accuracy: 0.0259\n",
      "Epoch 22/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.6191 - accuracy: 0.0306\n",
      "Epoch 23/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.5722 - accuracy: 0.0306\n",
      "Epoch 24/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.5181 - accuracy: 0.0235\n",
      "Epoch 25/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 4.4470 - accuracy: 0.0376\n",
      "Epoch 26/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.3722 - accuracy: 0.0376\n",
      "Epoch 27/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.2850 - accuracy: 0.0518\n",
      "Epoch 28/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.1845 - accuracy: 0.0729\n",
      "Epoch 29/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 4.0878 - accuracy: 0.0565\n",
      "Epoch 30/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 3.9682 - accuracy: 0.0706\n",
      "Epoch 31/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 3.8511 - accuracy: 0.1035\n",
      "Epoch 32/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 3.7540 - accuracy: 0.1012\n",
      "Epoch 33/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 3.5912 - accuracy: 0.1059\n",
      "Epoch 34/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 3.4534 - accuracy: 0.1247\n",
      "Epoch 35/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 3.3204 - accuracy: 0.1318\n",
      "Epoch 36/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 3.1961 - accuracy: 0.1671\n",
      "Epoch 37/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 3.0427 - accuracy: 0.1812\n",
      "Epoch 38/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 2.8648 - accuracy: 0.2706\n",
      "Epoch 39/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 2.7046 - accuracy: 0.2729\n",
      "Epoch 40/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 2.6265 - accuracy: 0.2659\n",
      "Epoch 41/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 2.5138 - accuracy: 0.2659\n",
      "Epoch 42/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 2.3481 - accuracy: 0.3459\n",
      "Epoch 43/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 2.1793 - accuracy: 0.3835\n",
      "Epoch 44/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 2.0364 - accuracy: 0.3976\n",
      "Epoch 45/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 1.9182 - accuracy: 0.4729\n",
      "Epoch 46/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 1.8313 - accuracy: 0.4400\n",
      "Epoch 47/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 1.6902 - accuracy: 0.4988\n",
      "Epoch 48/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 1.5689 - accuracy: 0.5482\n",
      "Epoch 49/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 1.5171 - accuracy: 0.5929\n",
      "Epoch 50/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 1.3615 - accuracy: 0.6118\n",
      "Epoch 51/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 1.3420 - accuracy: 0.6494\n",
      "Epoch 52/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 1.2579 - accuracy: 0.6541\n",
      "Epoch 53/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 1.1303 - accuracy: 0.7129\n",
      "Epoch 54/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 1.0054 - accuracy: 0.7176\n",
      "Epoch 55/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.8755 - accuracy: 0.7835\n",
      "Epoch 56/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.8428 - accuracy: 0.7812\n",
      "Epoch 57/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7896 - accuracy: 0.7859\n",
      "Epoch 58/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7318 - accuracy: 0.8141\n",
      "Epoch 59/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.8353\n",
      "Epoch 60/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5872 - accuracy: 0.8635\n",
      "Epoch 61/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.8871\n",
      "Epoch 62/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.8871\n",
      "Epoch 63/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.8612\n",
      "Epoch 64/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3664 - accuracy: 0.9271\n",
      "Epoch 65/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.9271\n",
      "Epoch 66/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.9341\n",
      "Epoch 67/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.9647\n",
      "Epoch 68/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2431 - accuracy: 0.9694\n",
      "Epoch 69/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.9435\n",
      "Epoch 70/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2174 - accuracy: 0.9647\n",
      "Epoch 71/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1991 - accuracy: 0.9765\n",
      "Epoch 72/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7611 - accuracy: 0.8518\n",
      "Epoch 73/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.9365\n",
      "Epoch 74/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2423 - accuracy: 0.9482\n",
      "Epoch 75/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.9671\n",
      "Epoch 76/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9859\n",
      "Epoch 77/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1279 - accuracy: 0.9835\n",
      "Epoch 78/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1629 - accuracy: 0.9788\n",
      "Epoch 79/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.9788\n",
      "Epoch 80/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1481 - accuracy: 0.9788\n",
      "Epoch 81/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1678 - accuracy: 0.9741\n",
      "Epoch 82/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1290 - accuracy: 0.9765\n",
      "Epoch 83/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1044 - accuracy: 0.9835\n",
      "Epoch 84/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1261 - accuracy: 0.9765\n",
      "Epoch 85/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1051 - accuracy: 0.9835\n",
      "Epoch 86/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1680 - accuracy: 0.9694\n",
      "Epoch 87/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.9812\n",
      "Epoch 88/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9859\n",
      "Epoch 89/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0718 - accuracy: 0.9859\n",
      "Epoch 90/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9835\n",
      "Epoch 91/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1135 - accuracy: 0.9812\n",
      "Epoch 92/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1278 - accuracy: 0.9835\n",
      "Epoch 93/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.9271\n",
      "Epoch 94/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.2247 - accuracy: 0.9647\n",
      "Epoch 95/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.9788\n",
      "Epoch 96/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1020 - accuracy: 0.9859\n",
      "Epoch 97/150\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9835\n",
      "Epoch 98/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9835\n",
      "Epoch 99/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9812\n",
      "Epoch 100/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9835\n",
      "Epoch 101/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0710 - accuracy: 0.9882\n",
      "Epoch 102/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.9835\n",
      "Epoch 103/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0824 - accuracy: 0.9882\n",
      "Epoch 104/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.9812\n",
      "Epoch 105/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9859\n",
      "Epoch 106/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9882\n",
      "Epoch 107/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9882\n",
      "Epoch 108/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9882\n",
      "Epoch 109/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9859\n",
      "Epoch 110/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9882\n",
      "Epoch 111/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9859\n",
      "Epoch 112/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9882\n",
      "Epoch 113/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0639 - accuracy: 0.9882\n",
      "Epoch 114/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9906\n",
      "Epoch 115/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0655 - accuracy: 0.9882\n",
      "Epoch 116/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9906\n",
      "Epoch 117/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 0.9859\n",
      "Epoch 118/150\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.0531 - accuracy: 0.9906\n",
      "Epoch 119/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9812\n",
      "Epoch 120/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0782 - accuracy: 0.9859\n",
      "Epoch 121/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0675 - accuracy: 0.9882\n",
      "Epoch 122/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9859\n",
      "Epoch 123/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9882\n",
      "Epoch 124/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9835\n",
      "Epoch 125/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0499 - accuracy: 0.9882\n",
      "Epoch 126/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0670 - accuracy: 0.9882\n",
      "Epoch 127/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9882\n",
      "Epoch 128/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9835\n",
      "Epoch 129/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0525 - accuracy: 0.9835\n",
      "Epoch 130/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9835\n",
      "Epoch 131/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0708 - accuracy: 0.9859\n",
      "Epoch 132/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9859\n",
      "Epoch 133/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9835\n",
      "Epoch 134/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9859\n",
      "Epoch 135/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9859\n",
      "Epoch 136/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9859\n",
      "Epoch 137/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0444 - accuracy: 0.9859\n",
      "Epoch 138/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0550 - accuracy: 0.9859\n",
      "Epoch 139/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9859\n",
      "Epoch 140/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9859\n",
      "Epoch 141/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9835\n",
      "Epoch 142/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0602 - accuracy: 0.9882\n",
      "Epoch 143/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9859\n",
      "Epoch 144/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9859\n",
      "Epoch 145/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9859\n",
      "Epoch 146/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9835\n",
      "Epoch 147/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0543 - accuracy: 0.9835\n",
      "Epoch 148/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9859\n",
      "Epoch 149/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0407 - accuracy: 0.9835\n",
      "Epoch 150/150\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9882\n",
      "Saving the Model.\n",
      "Pickle is also Saved.\n",
      "Loading Pickle.\n",
      "Loading the Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\STUDY\\Sem3\\DALAB3\\chatbot\\chatenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = tf.keras.optimizers.legacy.SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "print(\"Training.\")\n",
    "model.fit(np.array(train_x), np.array(train_y), epochs=150, batch_size=8, verbose=1)\n",
    "\n",
    "print(\"Saving the Model.\")\n",
    "model.save('model_keras.h5')\n",
    "\n",
    "print(\"Pickle is also Saved.\")\n",
    "pickle.dump({'words': words, 'classes': classes, 'train_x': train_x, 'train_y': train_y}, open(\"training_data\", \"wb\"))\n",
    "\n",
    "print(\"Loading Pickle.\")\n",
    "data = pickle.load(open(\"training_data\", \"rb\"))\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "print(\"Loading the Model.\")\n",
    "# load our saved model\n",
    "model = tf.keras.models.load_model('model_keras.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9906\n",
      "Loaded Model Accuracy: 0.9905882477760315\n"
     ]
    }
   ],
   "source": [
    "# Print loaded model accuracy\n",
    "loaded_model_loss, loaded_model_accuracy = model.evaluate(np.array(train_x), np.array(train_y))\n",
    "print(f\"Loaded Model Accuracy: {loaded_model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess user input\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a bag of words from the user input\n",
    "def bow(sentence, words, show_details=True):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"Found in bag: %s\" % w)\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the intent of the user input\n",
    "def predict_class(sentence, model, words, classes):\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence):\n",
    "    # Prediction or To Get the Possibility or Probability from the Model\n",
    "    results = model.predict(np.array([bow(sentence, words)]))[0]\n",
    "    # Exclude those results which are Below Threshold\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
    "    # Sorting is Done because higher Confidence Answer comes first.\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))  # Tuple -> Intent and Probability\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a response from the chatbot\n",
    "def response(sentence,userid='123',show_details=True):\n",
    "    results = classify(sentence)\n",
    "    # That Means if Classification is Done then Find the Matching Tag.\n",
    "    if results:\n",
    "        # Long Loop to get the Result.\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # Tag Finding\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # Random Response from High Order Probabilities\n",
    "                    return print(random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in bag: hi\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Hello, thanks for visiting\n",
      "Found in bag: hello\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Good to see you again\n",
      "Found in bag: suggest\n",
      "Found in bag: a\n",
      "Found in bag: movy\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Absolutely! I'd be happy to recommend a movie. What genre or mood are you in the mood for?\n",
      "Found in bag: tel\n",
      "Found in bag: a\n",
      "Found in bag: jok\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "What do you call fake spaghetti? An impasta.\n",
      "Found in bag: bye\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Bye! Come back again soon.\n"
     ]
    }
   ],
   "source": [
    "# Chatbot interaction loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    answer = response(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
